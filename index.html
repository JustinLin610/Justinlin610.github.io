<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Justin Lin</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<!-- [if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif] -->
		<script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        </script>
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<style>p{margin: 12px}</style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon fa-diamond"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Justin Lin</h1>
								<p><br>An Enthusiast for Linguistics and NLP.</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#intro">About</a></li>
								<li><a href="#pub">Pub</a></li>
								<li><a href="#blog">Blog</a></li>
								<li><a href="#cv">CV</a></li>
								<li><a href="#contact">Contact</a></li>
								<!-- <li><a href="#elements">Elements</a></li> -->
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">
								<h2 class="major">About Me</h2>
								<div id="container">
									<div id="photo">
										<p> </p>
										<img src="images/justin.jpg" alt="" width="120" />
									</div>
									<div id="content">
										<p style="font-size: 30px; font-family: Cambria, 'Hoefler Text', 'Liberation Serif', Times, 'Times New Roman', serif;"><strong>Junyang Lin (Justin) </strong></p>
										<p>Master Degree Candidate</p>
										<p>
											<a href="http://lanco.pku.edu.cn/index.html">Language Computing and Machine Learning Group</a>, <br>
											<a href="http://sfl.pku.edu.cn/">School of Foreign Languages</a>,<br>
											<a href="http://pku.edu.cn">Peking University</a><br>
										</p>
										<p><br><br></p>
									</div>
								</div>
                                <div id="container2">
									<p style="font-size: 25px"><strong>Intro:</strong></p>
									<p>I am Junyang (Justin) Lin, a graduate student in Institute of Linguistics and Applied Linguistics, School of Foreign Languages, Peking University. Since Nov. 2017, I have been working in the Language Computing and Machine Learning Group (LANCO), Key Laboratory of Computational Linguistics, supervised by Xu Sun and Qi Su. I have been studying Computational Linguistics for over a year and my major research focus is Deep Learning for Natural Language Processing, typically Neural Machine Translation and Abstractive Summarization. <br></p>
									<p><br></p>
									<p style="font-size: 25px"><strong>Research Interests:</strong></p>
									<p>Neural Machine Translation, Abstractive Summarization, Natural Language Generation</p>
									<p><br></p>
									<p style="font-size: 25px"><strong>Education:</strong></p>
									<p>Master of Linguistics and Applied Linguistics, Peking University</p>
									<p>Bachelor of English Literature and Culture, University of International Relations</p>
									<p ><br></p>
									<p style="font-size: 25px"><strong>Internship:</strong></p>
									<p>Language Computing and Machine Learning Group, Key Laboratory of Computational Linguistics, Peking University; Research Intern; Nov. 2017 till now</p>
									<!-- <p>Ogilvy and Mather, Public Relations; Intern in Automobile Group; Jan. 2016 - May 2016</p> -->
									<p><br></p>
									<p style="font-size: 25px"><strong>Recent News:</strong></p>
									<p>May 16, 2018. One paper (first author) was accepted by COLING 2018.</p>
									<p>Apr. 21, 2018. Three papers (1 as first author and 2 as co-author) were accepted in the ACL 2018</p>
									<p>Apr. 17, 2018. One paper (co-author) is accepted in the IJCAI.</p>
									<p>Feb. 6, 2018. Preprint "Decoding-History-Based Adaptive Control of Attention for Neural Machine Translation" (first author) was available on the <a href="https://arxiv.org/abs/1802.01812">arXiv</a></p>
									<p>Feb. 5, 2018. Preprint "DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text" (co-author) was available on the <a href="https://arxiv.org/abs/1802.01345">arXiv</a></p>
								</div>
							</article>

							<article id = "pub">
								<h2 class="major">Academics</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<p style="font-size: 25px"><strong>Publications:</strong></p>
								<p>
									Deconvolution-Based Global Decoding for Neural Machine Translation<br>
									<strong>Junyang Lin</strong>, Xu Sun, Xuancheng Ren, Shuming Ma, Jinsong Su and Qi Su<br>
									To appear in COLING 2018<br><br>
									Global Encoding for Abstractive Summarization [<a href="https://arxiv.org/abs/1805.03989">pdf</a>]<br>
                                    <strong>Junyang Lin</strong>, Xu Sun, Shuming Ma and Qi Su<br>
                                    To appear in ACL 2018<br><br>
                                    Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization<br>
                                    Shuming Ma, Xu Sun, <strong>Junyang Lin</strong> and Houfeng Wang<br>
                                    To appear in ACL 2018<br><br>
                                    Bag-of-Words as Target for Neural Machine Translation<br>
                                    Shuming Ma, Xu Sun, Yizhong Wang and <strong>Junyang Lin</strong><br>
                                    To appear in ACL 2018<br><br>
                                    A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification [<a href="https://arxiv.org/abs/1805.01089">pdf</a>]<br>
                                    Shuming Ma, Xu Sun, <strong>Junyang Lin</strong> and Xuancheng Ren<br>
                                    To appear in IJCAI 2018<br><br>
                                    Decoding-History-Based Adaptive Control of Attention for Neural Machine Translation [<a href="https://arxiv.org/abs/1802.01812">pdf</a>]<br>
                                    <strong>Junyang Lin</strong>, Shuming Ma, Qi Su and Xu Sun<br>
                                    arXiv<br><br>
                                    DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text [<a href="https://arxiv.org/abs/1802.01345">pdf</a>]<br>
                                    Jingjing Xu, Xu Sun, Xuancheng Ren, <strong>Junyang Lin</strong>, Bingzhen Wei and Wei Li<br>
                                    arXiv<br><br><br>
								</p>
								<p style="font-size: 25px"><strong>Technical Reports:</strong></p>
								<p>
									Learn the Dependencies Between Content Words and Function Words [<a href="http://shumingma.com/learn-dependencies-content.pdf">pdf</a>]<br>
									<strong>Junyang Lin</strong><br><br><br>
								</p>
								<p style="font-size: 25px"><strong>Academic Activities:</strong></p>
								<p>
									Reviewer for ACL 2018, IJCAI 2018, NAACL 2018, EMNLP 2017, CLSW 2017.<br>
									Reviewer for TALLIP.
								</p>
							</article>

						<!-- Work -->
							<article id="blog">
								<h2 class="major">Blog</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<p><a href="#DLPPT">Introduction to Deep Learning (PPT)</a>.</p>
								<p><a href="#SVM2">Another Perspective to View SVM (2)</a>.</p>
								<p><a href="#SVM">Another Perspective to View SVM (1)</a>.</p>
								
							</article>

							<article id="SVM">
								<h3 class="major">Another Perspective to View SVM (1)</h3>
								<!-- <span class="image main"><img src="images/pic02.jpg" alt="" /></span> -->
								<p>This is what I can never forget. Nine months ago, my mentor said to me:"Hey, how about building a paraphrasing system for your final project? Well try to do some feature engineering, label your data, and use SVM?" Ooh, maan, I can only write no more than ten lines of code at that time and you told me to build a system with a Support Vector Machine?! Is it a machine that I can afford? Er mar gerd... Back to our topic, since then, I have had a special feeling for SVM and I have always been curious about such black magic. In the following content, I am going to focus on the loss function of SVM and it involves some knowledge about square error and cross entropy.</p>
								<p>Support Vector Machine has nothing to do with a real machine. It is just a great machine learning algorithm which often has excellent performance in binary classification. Training a SVM model helps us find the best hyperplane (just like a plane in the high dimensional space) to separate the data and classify them to the right categories. Its main difference from perceptron is that it not only separates the data but also maximizes the margin. Here I do not want to discuss about details about functional margin and geometric margin. I try to illustrate it in a more intuitive way. Say, you have a bunch of data, like ten thousand documents with five thousand of positive attitude and another five thousand of negative attitude, and you want to train a model that can classify them. The procedure is relatively simple: you just need to turn the documents into feature vectors with labels, $+1$ for positive and $-1$ for negative. Suppose the data are linear separable, there should be a hyperplane that can best separate them with largest margin. Intuitively, on either side, there should be some vectors that are nearest to the hyperplane, which are support vectors. These vectors are on two hyperplanes that are parallel to the best hyperplane, $H_1$ and $H_2$, and the distance between $H_1$ and $H_2$ is margin. This is not difficult, but the problem is, how can we find such hyperplane? If you find some books to read, you will see a lot about convex quadratic programming using Lagrange function, which is really puzzling. However, I learned a lot from Hung-yi Lee, a professor in NTU, that training SVM can be viewed from another perspective, which I think is much simpler. If you know something about Pegasos, the fact that SVM can be trained with gradient descent may seem obvious to you. In the following parts, I am going to discuss about this issue.</p>
								<img src="images/SVM.jpg" alt="" />
								<p> </p>
								<p>Let's define the functions for linear separable SVM first. We have a bunch of feature vectors \(\mathcal{X} = \{x^{(1)}, x^{(2)}, ..., x^{(n)}\}\), corresponding to the label space \(\mathcal{Y} = \{+1, -1\}\), and now we use an affine function \(f(x) = w^{T}x + b\) to model the data points, and our task is to find the best hyperplane \(w^{*T}x + b^{*} = 0\) to separate them. For a certain data point $x^{(i)}$, if it is correctly classified, \(y^{(i)}f(x^{(i)}) > 0\). This is simple, but adding margin maximization problem to it makes it much more complicated. Instead of discussing about the functional and geometrical margin as well as convex quadratic programming, I would like to talk about the choice of loss function for a linear classifier.</p>
								<img src="images/loss_function.png" alt="" />
								<p>(From Hung-yi Lee's PPT in the Lecture 20 of Machine Learning)</p>
								<p>As is shown in the picture, the x axis represents $y^{(n)}f(x^{(n)})$, while the y axis represents the loss. In fact we have an ideal loss, which is \(l(g(x^{(n)}), y^{(n)}) = \delta(g(x^{(n)}) \neq y^{(n)})\), where \(g(x^{(n)}) = sign(f(x^{(n)}))\). However, it is problematic in application since it is totally not differentiable. Therefore, we need to find some alternatives, including L2 loss, sigmoid square loss, cross entropy loss and hinge loss. First of all, it is impossible to train a linear classifier with L2 loss function (\(l(f(x^{(n)}), y^{(n)}) = (y^{(n)}f(x^{(n)})-1)^2\), see the red curve). We hope that the target $y^{(n)}$ and the output $f(x^{(n)})$ are of the same sign, and we also hope that the larger their product is, the smaller the loss is, so using mean square error here is definitely inapproapriate. Second of all, sigmoid square loss is also not a good choice (\(l(f(x^{(n)}), y^{(n)}) = (\sigma(y^{(n)}f(x^{(n)}))-1)^2\)). If you have ever experienced the problem of vanishing gradient in your training of neural network, you can find that the problem here is similar. If the value $y^{(n)}f(x^{(n)})$ is a large negative number, its derivative is close to $0$, which means the gradient is too small for the loss to move downward. So how about cross entropy loss? I think it is a reasonable choice and its performance may be satisfactory. The loss function is \(l(f(x^{(n)}), y^{(n)}) = log(1 + exp(-y^{(n)}f(x^{(n)}))\). It seems great since it is the upper bound of the ideal loss and it is consistent with the rule "the larger value, the smaller loss". Moreover, when the $y^{(n)}f(x^{(n)})$ is negative, it has large gradient, which encourages the loss to move downward. However, is there even a better loss function? We may find that the problem of this function is that it encourages the value to be as large as possible, but in fact, if the value is larger than a certain number, our requirements are all satisfied and there is no need for it to be even larger. It may be not robust enough and may be influenced by the outliers.</p>
								<img src="images/hinge_loss.png" alt="" />
								<p>(From Andrew Zisserman's PPT in the Lecture 2 of Machine Learning)</p>
								<p>This function is pretty simple: $$l(f(x^{(n)}), y^{(n)}) = max(0, 1 - y^{(n)}f(x^{(n)}))$$ It means that when the value is negative, the loss is large and it encourages the value to become positive. Moreover, being positive is not enough and it should be larger than a certain value, here we set the value $1$. When the value is larger than 1, there is no need to decrease the loss because we now have enough confidence that we have correctly classified the data points, which solves the problem mentioned above. The value $1$ is actually our margin. You can try to prove it from the perspective of functional margin and geometrical margin. Now there is a problem about the value of the margin. The reason why it should be $1$ is that it makes the loss function a tight, or in another word, lowest upper bound of the ideal loss.</p>
								<p>Now that the problem becomes clear, we can build the final loss function for linear SVM: $$L(f) = \sum_{i=1}^{n}l(f(x^{(i)}), y^{(i)}) + \frac{\lambda}{2}||w||_2^2$$ Well, is it similar to the optimization problem that you are familiar with? $$min\frac{1}{2}||w||_2^2 + C\sum_{i=1}^{n}\xi^{(i)}$$ $$subject \; to \; y^{(i)}f(x^{(i)}) \geq 1 - \xi^{(i)}$$ Here the $\xi^{(i)}$ is our slack variable, which is also our hinge loss function. Wow, it is just like magic!</p>
								<p>I have to admit that the first time I learned these ideas I jumped off my chair excitedly. Even today I still feel that this perspective is really great for beginners to understand linear SVM. I hope I can share the happiness to you guys. Next time I will try to discuss about kernel tricks in SVM, starting from gradient descent of SVM.</p>
							</article>

							<article id="SVM2">
								<h3 class="major">Another Perspective to View SVM (2)</h3>
								<p>In this part, I am going to prove that $w$ is the linear combination of the inputs $x$. In the last passage, I discussed about the loss function of SVM and showed that it can be trained with gradient descent. Here I would like to present the training steps by calculating the gradients.</p>
								<p>The hinge loss function of SVM is: $$l(f(x^{(n)}), \hat{y}^{(n)})=max(0, 1 - \hat{y}^{(n)}f(x^{(n)}))$$ The derivative of $l(f(x^{(n)}), \hat{y}^{(n)})$ to $w_i$ can be calculated below: $$\frac{\partial{l}}{\partial{w_i}}=\frac{\partial{l}}{\partial{f}}\frac{\partial{f}}{\partial{w_i}}=\begin{cases} -\hat{y}^{(n)}x^{(n)}_i& 1-\hat{y}^{(n)}f(x^{(n)}) > 0 \\ 0& 1-\hat{y}^{(n)}f(x^{(n)}) \le 0 \end{cases}$$ so the gradient of $w_i$ is: $$\nabla{w_i}=\frac{\partial{L}}{\partial{w_i}}=\sum_{n}-\delta(\hat{y}^{(n)}f(x^{(n)})<1)\hat{y}^{(n)}x^{(n)}_{i}$$ and the gradient of the weight vector is: $$\nabla{w}=\frac{\partial{L}}{\partial{w}}=\sum_{n}-\delta(\hat{y}^{(n)}f(x^{(n)})<1)\hat{y}^{(n)}x^{(n)}$$ In the optimization procedure, the weight vector $w$ is optimized in each step in the following way: $$w=w-\eta\nabla{w}$$ If the weight vector reaches its global minimum in $m$ steps, which means the optimal $w^{*}$ is obtained, \nabla{w}=0. Therefore, if $w$ is initialized as $0$, $w^{*}$ can be presented as follows: $$w^{*}=m\cdot(-\eta\nabla{w})=\sum_{n}m\eta\delta(\hat{y}^{(n)}f(x^{(n)})<1)\hat{y}^{(n)}x^{(n)}$$ Now I define a term called $\alpha_{n}$, that: $$\alpha_{n}=m\eta\delta(\hat{y}^{(n)}f(x^{(n)})<1)$$ Then $w^{*}$ can be written as below: $$w^{*}=\sum_{n}\alpha_{n}\hat{y}^{(n)}x^{(n)}$$ This equation is the same of $w^{*}$ in the Lagrange function for solving the dual problem of SVM, and it proves that $w$ is a linear combination of the data points $x$. Here the equation can be further simplified to: $$w=\sum_{n}\alpha_{n}^{*}x^{(n)}=X\alpha^{*}$$ It should be noted that because of $\delta$ in $\alpha^{*}$, the vector $\alpha^{*}$ is sparse. The vectors $x^{(n)}$ with non-zero $\alpha_{n}^{*}$ are the support vectors holding the margin.</p>
								<p>Back to the affine function $f(x)=w^{T}x$, now it can be changed to: $$f(x)=\alpha^{T}X^{T}x=\sum_{n}\alpha_{n}(x_{n}\cdot{x})$$ Here I define a function called K that: $$K(x_{n}, x)=\sum_{n}x_{n}\cdot{x}$$ so: $$f(x)=\sum_{n}\alpha_{n}K(x_{n}\cdot{x})$$ Function K is called kernel function (here this K function is linear kernel function), which projects the input data from the input space to the feature space, usually of higher dimensions. The calculation of $K(x, z)$ is much simpler than projection function $\phi(x)$. Moreover, the power of kernel function is more than simple calculation. It is able to project the inputs from the inputs space to the space of infinite dimensions. One typical example is the well-known RBF function.</p>
								<p>In the next passage, I am going to introduce the traditional method to solve the primal problem of SVM, and try my best to interpret Lagrange multiplier, which has puzzled me for a long time, compared with L2 regularization.</p>
							</article>

							<article id="DLPPT">
								<h3 class="major">Intro to Deep Learning (PPT)</h3>
								<p>The following link is my PPT for a lecture in the course Computational Linguistics at our institute, which is a simple introduction to deep learning. If you are interested, <a href="DEEP LEARNING.pdf">please click here!</a></p>
							</article>

						<!-- About -->
							<article id="cv">
								<h2 class="major">CV</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<p>To be updated.</p>
							</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
<!-- 								<form method="post" action="#">
									<div class="field half first">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field half">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="4"></textarea>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="special" /></li>
										<li><input type="reset" value="Reset" /></li>
									</ul>
								</form> -->
								<ul class="icons">
									<li><a href="http://j.map.baidu.com/2jhxk" class="icon fa-map-marker"><span class="label">Address</span></a></li>
									<li><a href="mailto:linjunyang@pku.edu.cn" class="icon fa-envelope-square"><span class="label">Email</span></a></li>
									<li><a href="https://www.linkedin.com/in/%E4%BF%8A%E6%97%B8-%E6%9E%97-0b2b38151/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/JustinLin610" class="icon fa-github"><span class="label">GitHub</span></a></li>
								</ul>
								<p>Email: linjunyang@pku.edu.cn</p>
								<p>Address: No.5 Yiheyuan Rd., Haidian District, Beijing, China</p>
							</article>

						<!-- Elements -->
							<article id="elements">
								<h2 class="major">Elements</h2>

								<section>
									<h3 class="major">Text</h3>
									<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
									This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
									This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
									<hr />
									<h2>Heading Level 2</h2>
									<h3>Heading Level 3</h3>
									<h4>Heading Level 4</h4>
									<h5>Heading Level 5</h5>
									<h6>Heading Level 6</h6>
									<hr />
									<h4>Blockquote</h4>
									<blockquote>a a</blockquote>
									<h4>Preformatted</h4>
									<pre><code>i = 0;

while (!deck.isInOrder()) {
    print 'Iteration ' + i;
    deck.shuffle();
    i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
								</section>

								<section>
									<h3 class="major">Lists</h3>

									<h4>Unordered</h4>
									<ul>
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Alternate</h4>
									<ul class="alt">
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Ordered</h4>
									<ol>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis viverra.</li>
										<li>Felis enim feugiat.</li>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis lorem.</li>
										<li>Felis enim et feugiat.</li>
									</ol>
									<h4>Icons</h4>
									<ul class="icons">
										<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon fa-github"><span class="label">Github</span></a></li>
									</ul>

									<h4>Actions</h4>
									<ul class="actions">
										<li><a href="#" class="button special">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions vertical">
										<li><a href="#" class="button special">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Table</h3>
									<h4>Default</h4>
									<div class="table-wrapper">
										<table>
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>

									<h4>Alternate</h4>
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>
								</section>

								<section>
									<h3 class="major">Buttons</h3>
									<ul class="actions">
										<li><a href="#" class="button special">Special</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button">Default</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button special icon fa-download">Icon</a></li>
										<li><a href="#" class="button icon fa-download">Icon</a></li>
									</ul>
									<ul class="actions">
										<li><span class="button special disabled">Disabled</span></li>
										<li><span class="button disabled">Disabled</span></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Form</h3>
									<form method="post" action="#">
										<div class="field half first">
											<label for="demo-name">Name</label>
											<input type="text" name="demo-name" id="demo-name" value="" placeholder="Jane Doe" />
										</div>
										<div class="field half">
											<label for="demo-email">Email</label>
											<input type="email" name="demo-email" id="demo-email" value="" placeholder="jane@untitled.tld" />
										</div>
										<div class="field">
											<label for="demo-category">Category</label>
											<div class="select-wrapper">
												<select name="demo-category" id="demo-category">
													<option value="">-</option>
													<option value="1">Manufacturing</option>
													<option value="1">Shipping</option>
													<option value="1">Administration</option>
													<option value="1">Human Resources</option>
												</select>
											</div>
										</div>
										<div class="field half first">
											<input type="radio" id="demo-priority-low" name="demo-priority" checked>
											<label for="demo-priority-low">Low</label>
										</div>
										<div class="field half">
											<input type="radio" id="demo-priority-high" name="demo-priority">
											<label for="demo-priority-high">High</label>
										</div>
										<div class="field half first">
											<input type="checkbox" id="demo-copy" name="demo-copy">
											<label for="demo-copy">Email me a copy</label>
										</div>
										<div class="field half">
											<input type="checkbox" id="demo-human" name="demo-human" checked>
											<label for="demo-human">Not a robot</label>
										</div>
										<div class="field">
											<label for="demo-message">Message</label>
											<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
										</div>
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="special" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</form>
								</section>

							</article>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<!-- <p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p> -->
						<p class="copyright">&copy; Justin Lin </p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
